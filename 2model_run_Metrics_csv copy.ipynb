{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b826ce53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-28 19:39:44.914603: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-11-28 19:39:50.248723: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-11-28 19:40:18.143264: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:31] Could not find cuda drivers on your machine, GPU will not be used.\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import joblib\n",
    "import sys\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src import (\n",
    "    DataPreprocessor,\n",
    "    train_linear_models,\n",
    "    train_random_forest,\n",
    "    train_boosting_models,\n",
    "    train_mlp,\n",
    "    ModelComparator,\n",
    "    FeatureImportanceAnalyzer,\n",
    "    load_all_data,\n",
    "    prepare_features_target,\n",
    "    save_all_results,\n",
    "    print_data_summary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e0da0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all data TESTING METRICS DATASET\n",
    "metrics_df = pd.read_csv('Data/VED_DynamicData_Metrics.csv')\n",
    "static_ice_hev = pd.read_csv('Data/VED_Static_Data_ICE&HEV.csv')\n",
    "static_phev_ev = pd.read_csv('Data/VED_Static_Data_PHEV&EV.csv')\n",
    "\n",
    "print(f\"Metrics CSV: {metrics_df.shape}\")\n",
    "print(f\"Static ICE/HEV: {static_ice_hev.shape}\")\n",
    "print(f\"Static PHEV/EV: {static_phev_ev.shape}\")\n",
    "\n",
    "# Check columns in metrics_df\n",
    "print(f\"\\nMetrics CSV columns (first 10): {metrics_df.columns.tolist()[:10]}\")\n",
    "print(f\"Static data columns: {static_ice_hev.columns.tolist()}\")\n",
    "\n",
    "# Combine static data\n",
    "static_data = pd.concat([static_ice_hev, static_phev_ev], ignore_index=True)\n",
    "print(f\"Combined static data: {static_data.shape}\")\n",
    "\n",
    "if 'filename' in metrics_df.columns and 'VehId' in static_data.columns:\n",
    "    print(\"\\nMetrics has 'filename' but not 'VehId'. Need to match vehicles differently.\")\n",
    "    print(\"For now, using all rows from metrics (54 vehicle-weeks)\")\n",
    "    data = metrics_df.copy()\n",
    "    print(f\"Using metrics data: {data.shape[0]} samples, {data.shape[1]} features\")\n",
    "else:\n",
    "    # Try standard merge on VehId\n",
    "    data = metrics_df.merge(static_data, on='VehId', how='inner')\n",
    "    print(f\"Merged data: {data.shape[0]} samples, {data.shape[1]} features\")\n",
    "\n",
    "print(f\"\\nData shape: {data.shape}\")\n",
    "print(f\"Data columns (first 15): {data.columns.tolist()[:15]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb474d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define exclusion columns (exclude from features, but keep VehId for identification)\n",
    "exclude_cols = ['filename', 'Trip', 'Timestamp', 'Timestamp(ms)', \n",
    "                'DayNum_mean', 'DayNum_median', 'DayNum_std', 'DayNum_min', 'DayNum_max',\n",
    "                'DayNum_q25', 'DayNum_q75', 'DayNum_sum', 'DayNum_count', 'DayNum_mode',\n",
    "                'DayNum_mode_count', 'DayNum_range', 'DayNum_iqr', 'DayNum_cv']\n",
    "\n",
    "# Keep VehId for identification\n",
    "veh_ids = data['VehId'].values if 'VehId' in data.columns else None\n",
    "\n",
    "# Get numeric feature columns (excluding non-numeric and identifiers)\n",
    "feature_cols = [col for col in data.columns \n",
    "               if col not in exclude_cols \n",
    "               and col != 'VehId'  # Explicitly exclude VehId from features\n",
    "               and data[col].dtype in [np.float64, np.int64]]\n",
    "\n",
    "print(f\"Total features selected: {len(feature_cols)}\")\n",
    "print(f\"First 10 features: {feature_cols[:10]}\")\n",
    "print(f\"VehId preserved for vehicle identification: {veh_ids is not None}\")\n",
    "\n",
    "# Prepare X and y\n",
    "X = data[feature_cols].fillna(data[feature_cols].mean())\n",
    "\n",
    "# Find a suitable target column (numeric, not an identifier)\n",
    "# Try to find engine-related features or use the first available numeric column\n",
    "possible_targets = [col for col in feature_cols if 'EngDisp' in col or 'Engine' in col]\n",
    "if possible_targets:\n",
    "    target_col = possible_targets[0]\n",
    "else:\n",
    "    # Use first meaningful feature as target\n",
    "    target_col = feature_cols[0]\n",
    "\n",
    "y = data[target_col].fillna(data[target_col].mean()).values\n",
    "\n",
    "print(f\"\\nFeature matrix shape: {X.shape}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Target - Mean: {y.mean():.4f}, Std: {y.std():.4f}, Min: {y.min():.4f}, Max: {y.max():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f8d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split: 80% dev (for train+val), 20% test\n",
    "# Then dev split: 80% train, 20% val from dev\n",
    "\n",
    "# Create indices for splitting while preserving VehId\n",
    "indices = np.arange(len(X))\n",
    "\n",
    "# First split: dev (80%) vs test (20%)\n",
    "idx_dev, idx_test, X_dev, X_test, y_dev, y_test = train_test_split(\n",
    "    indices, X, y, test_size=0.2, random_state=69\n",
    ")\n",
    "\n",
    "# Get VehIds for each split if available\n",
    "if veh_ids is not None:\n",
    "    veh_ids_dev = veh_ids[idx_dev]\n",
    "    veh_ids_test = veh_ids[idx_test]\n",
    "else:\n",
    "    veh_ids_dev = None\n",
    "    veh_ids_test = None\n",
    "\n",
    "print(f\"Dev set: {X_dev.shape[0]} samples ({len(X_dev)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Second split: dev into train (80% of dev ≈ 64% of total) vs val (20% of dev ≈ 16% of total)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_dev, y_dev, test_size=0.2, random_state=69\n",
    ")\n",
    "\n",
    "if veh_ids_dev is not None:\n",
    "    veh_ids_train = veh_ids_dev[:len(X_train)]\n",
    "    veh_ids_val = veh_ids_dev[len(X_train):]\n",
    "else:\n",
    "    veh_ids_train = None\n",
    "    veh_ids_val = None\n",
    "\n",
    "print(f\"Train set: {X_train.shape[0]} samples ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"Val set: {X_val.shape[0]} samples ({len(X_val)/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "\n",
    "# Verify split\n",
    "total = len(X_train) + len(X_val) + len(X_test)\n",
    "print(f\"\\nTotal samples: {total}\")\n",
    "print(f\"Train: {len(X_train)/total*100:.1f}% | Val: {len(X_val)/total*100:.1f}% | Test: {len(X_test)/total*100:.1f}%\")\n",
    "\n",
    "# Store VehId information for later use\n",
    "print(f\"\\nVehId information preserved:\")\n",
    "if veh_ids_train is not None:\n",
    "    print(f\"Train VehIds: {len(veh_ids_train)} cars\")\n",
    "    print(f\"Val VehIds: {len(veh_ids_val)} cars\")\n",
    "    print(f\"Test VehIds: {len(veh_ids_test)} cars\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac2038f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create scaler and fit on training data ONLY\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Scale all sets\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convert to DataFrames for easier handling (optional)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=feature_cols, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=feature_cols, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=feature_cols, index=X_test.index)\n",
    " \n",
    "# Sanitize feature names for XGBoost (remove special characters)\n",
    "sanitized_feature_cols = [col.replace('[', '').replace(']', '').replace('<', '').replace('>', '') for col in feature_cols]\n",
    "X_train_scaled.columns = sanitized_feature_cols\n",
    "X_val_scaled.columns = sanitized_feature_cols\n",
    "X_test_scaled.columns = sanitized_feature_cols\n",
    "\n",
    "print(f\"Train set shape: {X_train_scaled.shape}\")\n",
    "print(f\"Val set shape: {X_val_scaled.shape}\")\n",
    "print(f\"Test set shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nFeature statistics after scaling (train):\")\n",
    "print(f\"Mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"Std: {X_train_scaled.std().mean():.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128726c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred):\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100 if np.all(y_true != 0) else np.nan\n",
    "    return {'RMSE': rmse, 'MAE': mae, 'R2': r2, 'MAPE': mape}\n",
    "\n",
    "all_models = {}\n",
    "all_predictions = {}\n",
    "\n",
    "# -------- Linear OLS --------\n",
    "print(\"\\n→ Training Linear OLS...\")\n",
    "model_ols = LinearRegression()\n",
    "model_ols.fit(X_train_scaled, y_train)\n",
    "preds_ols = model_ols.predict(X_test_scaled)\n",
    "\n",
    "all_models['Linear OLS'] = model_ols\n",
    "all_predictions['Linear OLS'] = preds_ols\n",
    "metrics = calculate_metrics(y_test, preds_ols)\n",
    "print(f\"Trained - RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "# -------- Linear Ridge --------\n",
    "print(\"\\n→ Training Linear Ridge...\")\n",
    "model_ridge = Ridge(alpha=1.0, random_state=42)\n",
    "model_ridge.fit(X_train_scaled, y_train)\n",
    "preds_ridge = model_ridge.predict(X_test_scaled)\n",
    "\n",
    "all_models['Linear Ridge'] = model_ridge\n",
    "all_predictions['Linear Ridge'] = preds_ridge\n",
    "metrics = calculate_metrics(y_test, preds_ridge)\n",
    "print(f\"Trained - RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "# -------- Linear Lasso --------\n",
    "print(\"\\n→ Training Linear Lasso...\")\n",
    "model_lasso = Lasso(alpha=0.01, random_state=42, max_iter=10000)\n",
    "model_lasso.fit(X_train_scaled, y_train)\n",
    "preds_lasso = model_lasso.predict(X_test_scaled)\n",
    "\n",
    "all_models['Linear Lasso'] = model_lasso\n",
    "all_predictions['Linear Lasso'] = preds_lasso\n",
    "metrics = calculate_metrics(y_test, preds_lasso)\n",
    "print(f\"Trained - RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "\n",
    "# -------- Random Forest --------\n",
    "print(\"\\n→ Training Random Forest...\")\n",
    "model_rf = RandomForestRegressor(n_estimators=200, max_depth=20, min_samples_split=5, random_state=42, n_jobs=-1)\n",
    "model_rf.fit(X_train_scaled, y_train)\n",
    "preds_rf = model_rf.predict(X_test_scaled)\n",
    "\n",
    "all_models['Random Forest'] = model_rf\n",
    "all_predictions['Random Forest'] = preds_rf\n",
    "metrics = calculate_metrics(y_test, preds_rf)\n",
    "print(f\"Trained - RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "# -------- XGBoost --------\n",
    "print(\"\\n→ Training XGBoost...\")\n",
    "model_xgb = xgb.XGBRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=7, \n",
    "    random_state=42, verbosity=0\n",
    ")\n",
    "model_xgb.fit(X_train_scaled, y_train, eval_set=[(X_val_scaled, y_val)], verbose=False)\n",
    "preds_xgb = model_xgb.predict(X_test_scaled)\n",
    "\n",
    "all_models['XGBoost'] = model_xgb\n",
    "all_predictions['XGBoost'] = preds_xgb\n",
    "metrics = calculate_metrics(y_test, preds_xgb)\n",
    "print(f\"Trained - RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "# -------- LightGBM --------\n",
    "print(\"\\n→ Training LightGBM...\")\n",
    "model_lgbm = lgb.LGBMRegressor(\n",
    "    n_estimators=300, learning_rate=0.05, max_depth=7,\n",
    "    random_state=42, verbose=-1\n",
    ")\n",
    "model_lgbm.fit(X_train_scaled, y_train)\n",
    "preds_lgbm = model_lgbm.predict(X_test_scaled)\n",
    "\n",
    "all_models['LightGBM'] = model_lgbm\n",
    "all_predictions['LightGBM'] = preds_lgbm\n",
    "metrics = calculate_metrics(y_test, preds_lgbm)\n",
    "print(f\"Trained - RMSE: {metrics['RMSE']:.4f}, R²: {metrics['R2']:.4f}\")\n",
    "\n",
    "print(f\"Models: {list(all_models.keys())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a7e2f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "comparator = ModelComparator()\n",
    "\n",
    "for model_name, predictions in all_predictions.items():\n",
    "    comparator.add_model_results(model_name, y_test, predictions)\n",
    "\n",
    "comparator.print_summary()\n",
    "\n",
    "comparison_df = comparator.get_comparison_dataframe()\n",
    "print(\"\\nDetailed Comparison Table:\")\n",
    "print(comparison_df.round(6).to_string())\n",
    "\n",
    "comparison_df.to_csv('results/model_comparison.csv')\n",
    "\n",
    "if veh_ids_test is not None:\n",
    "    detailed_results = pd.DataFrame({\n",
    "        'VehId': veh_ids_test,\n",
    "        'Actual': y_test\n",
    "    })\n",
    "    \n",
    "    for model_name, predictions in all_predictions.items():\n",
    "        detailed_results[f'{model_name}_Pred'] = predictions\n",
    "    \n",
    "    detailed_results.to_csv('results/model_predictions_with_vehicles.csv', index=False)\n",
    "    print(f\"Detailed predictions with vehicle IDs saved to results/model_predictions_with_vehicles.csv\")\n",
    "    print(f\"Samples: {len(detailed_results)} test vehicles\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b9c296",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_path = '/media/vanafa/1TB/Workspace/Cuatri 6/ML/TP Final'\n",
    "if project_path not in sys.path:\n",
    "    sys.path.insert(0, project_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e0d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (15, 5)\n",
    "\n",
    "# 1. Model Comparison - RMSE\n",
    "fig = comparator.plot_comparison(metrics=['RMSE', 'MAE', 'R2'], figsize=(15, 5))\n",
    "plt.suptitle('Model Comparison Across Metrics', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/model_comparison_metrics.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 2. Predictions vs Actual for top 3 models\n",
    "best_model_name, best_rmse = comparator.get_best_model('RMSE')\n",
    "print(f\"\\nBest model: {best_model_name} (RMSE: {best_rmse:.6f})\")\n",
    "\n",
    "# Get top 3 models by RMSE\n",
    "top_3_models = comparison_df['RMSE'].nsmallest(3).index.tolist()\n",
    "top_3_preds = {name: all_predictions[name] for name in top_3_models}\n",
    "\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for idx, (model_name, preds) in enumerate(top_3_preds.items(), 1):\n",
    "    ax = plt.subplot(1, 3, idx)\n",
    "    \n",
    "    r2 = comparison_df.loc[model_name, 'R2']\n",
    "    rmse = comparison_df.loc[model_name, 'RMSE']\n",
    "    \n",
    "    ax.scatter(y_test, preds, alpha=0.6, s=50)\n",
    "    \n",
    "    # Perfect prediction line\n",
    "    min_val = min(y_test.min(), preds.min())\n",
    "    max_val = max(y_test.max(), preds.max())\n",
    "    ax.plot([min_val, max_val], [min_val, max_val], 'r--', lw=2, label='Perfect')\n",
    "    \n",
    "    ax.set_xlabel('Actual')\n",
    "    ax.set_ylabel('Predicted')\n",
    "    ax.set_title(f'{model_name}\\nR² = {r2:.4f}, RMSE = {rmse:.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/predictions_vs_actual_top3.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 3. Residuals plot for top 3 models\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for idx, (model_name, preds) in enumerate(top_3_preds.items(), 1):\n",
    "    ax = plt.subplot(1, 3, idx)\n",
    "    \n",
    "    residuals = y_test - preds\n",
    "    \n",
    "    ax.scatter(preds, residuals, alpha=0.6, s=50)\n",
    "    ax.axhline(y=0, color='r', linestyle='--', lw=2)\n",
    "    \n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_ylabel('Residuals')\n",
    "    ax.set_title(f'{model_name}\\nMean Res = {residuals.mean():.4f}')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/residuals_top3.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# 4. Error distribution for top 3 models\n",
    "fig = plt.figure(figsize=(15, 5))\n",
    "for idx, (model_name, preds) in enumerate(top_3_preds.items(), 1):\n",
    "    ax = plt.subplot(1, 3, idx)\n",
    "    \n",
    "    errors = np.abs(y_test - preds)\n",
    "    \n",
    "    ax.hist(errors, bins=30, edgecolor='black', alpha=0.7, color='skyblue')\n",
    "    ax.axvline(errors.mean(), color='r', linestyle='--', lw=2, label=f'Mean: {errors.mean():.4f}')\n",
    "    ax.axvline(np.median(errors), color='g', linestyle='--', lw=2, label=f'Median: {np.median(errors):.4f}')\n",
    "    \n",
    "    ax.set_xlabel('Absolute Error')\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'{model_name}')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('results/error_distribution_top3.png', dpi=100, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3ed14e",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = FeatureImportanceAnalyzer(feature_cols)\n",
    "\n",
    "if hasattr(all_models['Random Forest'], 'get_feature_importance'):\n",
    "    importance = all_models['Random Forest'].get_feature_importance()\n",
    "    analyzer.add_importance('Random Forest', importance, 'importance')\n",
    "\n",
    "if hasattr(all_models['XGBoost'], 'get_feature_importance'):\n",
    "    importance = all_models['XGBoost'].get_feature_importance()\n",
    "    analyzer.add_importance('XGBoost', importance, 'importance')\n",
    "\n",
    "if hasattr(all_models['LightGBM'], 'get_feature_importance'):\n",
    "    importance = all_models['LightGBM'].get_feature_importance()\n",
    "    analyzer.add_importance('LightGBM', importance, 'importance')\n",
    "\n",
    "if hasattr(all_models['Linear OLS'], 'coef_'):\n",
    "    analyzer.add_importance('Linear OLS', np.abs(all_models['Linear OLS'].coef_), 'coefficient')\n",
    "\n",
    "analyzer.print_summary(n=15)\n",
    "\n",
    "for model_name in ['Random Forest', 'XGBoost', 'LightGBM']:\n",
    "    if model_name in analyzer.importances:\n",
    "        fig = analyzer.plot_feature_importance(model_name, n=15, figsize=(10, 8))\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'results/feature_importance_{model_name.lower().replace(\" \", \"_\")}.png', \n",
    "                    dpi=100, bbox_inches='tight')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0149605",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_name, best_rmse = comparator.get_best_model('RMSE')\n",
    "best_mae = comparator.get_best_model('MAE')[1]\n",
    "best_r2 = comparator.get_best_model('R2')[1]\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "print(f\"RMSE: {best_rmse:.6f}\")\n",
    "print(f\"MAE:  {best_mae:.6f}\")\n",
    "print(f\"R²:   {best_r2:.6f}\")\n",
    "\n",
    "print(\"\\nMODEL RANKING (by RMSE):\")\n",
    "ranking = comparison_df.sort_values('RMSE')\n",
    "for i, (model, row) in enumerate(ranking.iterrows(), 1):\n",
    "    print(f\"{i}. {model:20s} | RMSE: {row['RMSE']:10.6f} | MAE: {row['MAE']:10.6f} | R²: {row['R2']:8.6f}\")\n",
    "\n",
    "print(\"METRICS INTERPRETATION\")\n",
    "print(\"\"\"\n",
    "RMSE (Root Mean Squared Error):\n",
    "  - Average magnitude of prediction errors\n",
    "  - Lower is better\n",
    "  - Same units as target variable\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "  - Average absolute prediction error\n",
    "  - More interpretable than RMSE\n",
    "  - Lower is better\n",
    "\n",
    "R² (Coefficient of Determination):\n",
    "  - Proportion of variance explained by model\n",
    "  - 1.0 = perfect, 0.0 = no better than mean\n",
    "  - Higher is better\n",
    "\n",
    "MAPE (Mean Absolute Percentage Error):\n",
    "  - Percentage error relative to actual values\n",
    "  - Useful for comparing across scales\n",
    "  - Lower is better (%)\n",
    "\"\"\")\n",
    "\n",
    "best_model = all_models[best_model_name]\n",
    "if hasattr(best_model, 'save_model'):\n",
    "    best_model.save_model(f'models/saved_models/best_model_{best_model_name.replace(\" \", \"_\")}.pkl')\n",
    "\n",
    "for model_name, model in all_models.items():\n",
    "    if hasattr(model, 'save_model'):\n",
    "        ext = '.h5' if 'MLP' in model_name else '.pkl'\n",
    "        model.save_model(f'models/saved_models/{model_name.replace(\" \", \"_\")}{ext}')\n",
    "\n",
    "joblib.dump(scaler, 'models/saved_models/feature_scaler.pkl')\n",
    "\n",
    "print(f\"\"\"\n",
    "Results saved to:\n",
    "  - results/model_comparison.csv (metrics table)\n",
    "  - results/model_comparison_metrics.png (comparison plot)\n",
    "  - results/predictions_vs_actual_top3.png (predictions plot)\n",
    "  - results/residuals_top3.png (residuals plot)\n",
    "  - results/error_distribution_top3.png (error distribution)\n",
    "  - results/feature_importance_*.png (feature importance plots)\n",
    "\n",
    "Models saved to:\n",
    "  - models/saved_models/best_model_*.pkl\n",
    "  - models/saved_models/*_model.pkl/.h5\n",
    "  - models/saved_models/feature_scaler.pkl\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
